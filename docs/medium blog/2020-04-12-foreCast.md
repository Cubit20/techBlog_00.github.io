---
layout: post
title : Amazon Forecast로 코로나 바이러스 동향 예측하기
category : medium blog
order : 7
date: 2020-04-12
---

![그림1](/assets/images/mediumBlog/20.04.12-foreCast/foreCast_01.jpg)

우선 지금 이 시간에도 코로나 바이러스와 치열한 사투를 벌이고 계신 의료진, 구급대원, 자원봉사자, 공무원분들 및 사회적 거리두기로 코로나 극복에 일조를 하고 계시는 모든 분들 응원합니다!

지난 re:Invent 2018 에서 Amazon Web Services (AWS)는 Amazon.com에 사용되는 기계학습 기술을 기반으로 한 시계열 예측 서비스인 Amazon Forecast를 발표했고, 작년 8월부터 정식서비스를 시작했습니다.

여기서 시계열 예측(Time Series Forecasting)이란 "과거의 데이터와 현재의 값을 기반으로 미래의 값"을 예측하는 것 입니다. 예를 들자면 쿠팡 같은 이커머스(E-Commerce) 업체들은 물류 효율성을 극대화 하기 위해서 판매하는 상품들에 대한 미래 수요 예측을 해서 적당량의 재고를 물류 창고에 확보를 하고 상품을 배송할 직원을 채용해야 할 것입니다.

이 뿐만 아니라 저희들도 일상생활에서 시계열 예측을 하고 있습니다. 뇌라는 슈퍼컴퓨터가 기억속에 저장되어 있는 과거의 경험들을 기반으로 미래의 사건을 예측하곤 합니다. 예를 들자면 영수라는 친구를 10년동안 알고 지냈는데 이 친구가 항상 약속시간보다 10분 늦게 왔다면 저희는 이 친구가 다음 약속에서도 10분 늦게 올거라고 높은 확률로 예측할수 있을 것 입니다. 또는 새로 사귄 친구가 이 친구랑 비슷한 성격을 가지고 있다면 이 친구 역시 약속에 늦을 확률이 높다고 볼수 있을 겁니다.

이번 포스팅에서는 전국 지역별(특별시, 광역시, 도)로 앞으로의 코로나 발생 동향을 예측해보겠습니다

## Dataset

미래를 예측하려면 과거 데이터가 필요합니다. Kaggle에 올라와 있는 국내 코로나 발생 현황 데이터를 이용하도록 하겠습니다.

![그림2](/assets/images/mediumBlog/20.04.12-foreCast/foreCast_02.png)

TimeProvince.csv 파일에서 2020년 1월 20일 부터 2020년 4월 7일까지 전국 지역별 누적 확진자, 완치자, 사망자 수치에 대한 일별 데이터를 확인할수 있습니다. 신규 확진자에 대한 예측 모델을 생성할 예정이라서 데이터 전처리 과정을 통해서 데이터를 누적 확진자 수치가 아닌 신규 확진자 수치로 변경하였습니다. 또한 해당 데이터에는 입국검역을 통한 확진자 수치를 포함하고 있지 않습니다. 2020년 4월 7일 기준 누적확진자 총합은 10,331명이지만 검역을 통한 확진자 324명에 대한 데이터를 제외한 총 10,007명의 확진자 데이터만 해당 데이터셋에 포함하고 있습니다. 실제 Training에 사용된 데이터는 해당 [Git Repository](https://github.com/youngwjung/covid-forecast/blob/master/TimeProvince.csv)에서 확인 가능합니다.

## 예측 모델 생성하기

우선 Dataset group를 생성해야 하는데요. Dataset group이란 예측 모델 학습에 필요한 Datasets, 학습된 예측 모델인 Preditors, 예측 모델이 배포되어 예측 Query를 실행하는 Forecasts가 구성되는 하나의 논리적 그룹이라고 생각하시면 될 거 같습니다.

![그림3](/assets/images/mediumBlog/20.04.12-foreCast/foreCast_03.png)

Dataset group를 생성할 때 Forecasting domain을 지정해주어야 합니다. Forecasting domain에는 예측 모델의 주요 사용 사례들인 소매 수요, 재고, EC2 사용량, 인력, 웹 트래픽, 지표(수익, 판매, 현금흐름)과 사용자 지정 도메인이 있습니다. 각 도메인별로 요구되는 Dataset 필드가 다르기 때문에 [AWS 공식문서](https://docs.aws.amazon.com/ko_kr/forecast/latest/dg/howitworks-domains-ds-types.html)에 명시된 각 도메인별 Dataset 필드를 확인하시고 도메인을 지정하셔야 합니다. 코로나 예측 같은 경우에는 미리 정의된 도메인들과 성격이 다르므로 사용자 지정 도메인으로 선택하겠습니다.

다음으로는 시계열 Dataset을 생성합니다.

![그림4](/assets/images/mediumBlog/20.04.12-foreCast/foreCast_04.png)

일일 코로나 현황 데이터이므로 Time Interval은 1 day로 지정하겠습니다. Data schema에는 앞에서 지정한 도메인에서 요구되는 Dataset 필드가 자동으로 들어가 있습니다. 여기서 주의하실 부분은 CSV 파일에 저장된 데이터 필드들의 순서와 Data schema에 정의된 Attribute 순서가 동일해야 합니다. 위의 예로 들자면 CSV 파일이 아래와 같아야 합니다.

    timestamps, item_id, target_value
    2020-01-20, Seoul, 0
    ...
    ...
    2002-04-07, Seoul,

다음으로는 학습에 사용할 데이터를 Dataset으로 불러오는 작업입니다.

![그림5](/assets/images/mediumBlog/20.04.12-foreCast/foreCast_05.png)

Timestamp format 에 불러올 데이터의 timestamp 필드에 정의된 포맷을 넣어줍니다. S3 버킷을 생성하거나 기존의 생성된 csv 파일을 업로드하고 Data Location에 경로를 넣어줍니다. S3에서 파일을 읽어올수 있는 권한이 필요하므로 IAM Role 필드에서 Create a new role를 선택하고 csv 파일이 저장된 버킷에 대한 권한을 부여합니다.

![그림6](/assets/images/mediumBLog/20.04.12-foreCast/foreCast_06.png)

자 이제 데이터 불러오는 작업이 진행중이구요.

![그림7](/assets/images/mediumBlog/20.04.12-foreCast/foreCast_07.png)

잠시만 기다리면 작업이 완료됩니다.

![그림8](/assets/images/mediumBlog/20.04.12-foreCast/foreCast_08.png)

예측 모델을 생성하기 앞서 Dataset Type에 대해서 간략히 알아보겠습니다.

![그림9](/assets/images/mediumBlog/20.04.12-foreCast/foreCast_09.png)

TARGET_TIME_SERIES는 예측 모델 생성에 필수적인 Dataset으로서 처음에 Dataset group을 생성할때 만든 Dataset이 바로 TARGET_TIME_SERIES Dataset입니다. 이 Dataset에서는 예측할 아이템 값의 시계열 데이터가 포함되어야 합니다. 예로 스타벅스 각 지점별 일일 커피 판매량 또는 일일 특정 커피 판매량를 예측 한다고 하면 아래와 같은 데이터가 TARGET_TIME_SERIES로 필요할 것입니다.

    timestamp, branch_id, item_id, amount_sold

ITEM_METADATA에는 예측할 아이템의 특성에 대한 정보가 포함되어야 합니다. 위의 스타벅스의 예로 말한다면 특정 커피의 카페인 함유 유/무, 우유 함유 유/무, 원두 원산지, 핫/아이스 등이 될 수 있을거 같습니다. 여기서 유의할 점은 ITEM_METADATA의 필드 값은 문자열만 허용하고, Deep AR+ 알고리즘으로 예측 모델을 생성할때면 ITEM_METADATA Dataset이 모델 학습에 이용됩니다. 이 부분에 대한 자세한 내용을 알고리즘 소개할 때 알아보겠습니다.

RELATED_TIME_SERIES에는 TARGET_TIME_SERIES에서 명시한 아이템에 관련된 다른 시계열 데이터가 포함되어야 합니다. 위의 스타벅스의 예로 말한다면 아래의 데이터와 같은 각 아이템별 가격이나 프로모션 유/무가 될 수 있겠네요.

    timestamp, item_id, price, promotion

***

자 이제 불러온 Dataset으로 예측 모델을 만들어 보도록 하겠습니다.

![그림10](/assets/images/mediumBlog/20.04.12-foreCast/foreCast_10.png)

Forecast horizon에는 Dataset에 있는 가장 최근 날짜를 기준으로 얼마나 먼 미래의 값을 예측하고 싶은지를 정합니다. 저는 7일로 설정했는데요. 이말인 즉슨 현재 불러온 Dataset에 2020년 4월 7일까지의 데이터가 있으니 생성된 예측 모델로 4월 8일부터 4월 14일까지의 값을 예측할 수 있습니다.

Forecast frequency에는 얼마 주기로 예측을 할건지를 정합니다. 저는 일별 코로나 확진자 수를 예측할거니까 1 day로 설정했습니다. 만약 주별 확진자 수를 예측하려고 한다면 week으로 지정하면 됩니다. Forecast frequency 값은 Dataset에 있는 timestamp 간격보다 크게 설정할수는 있지만 더 작게 설정할수는 없습니다. 예를 들자면 Dataset의 timestamp 간격이 1 hour라면 Forecast frequency 값을 day, week, month로 지정할수 있지만, timestamp 간격이 1 week이라면 Forecast frequency 값을 hour, day로 지정할 수 없습니다.

Algorithm selection 에서는 Training에 사용할 아록리즘을 정할 수 있습니다. 해당 [문서](https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-choosing-recipes.html)를 참고해서 특정 알고리즘을 선택하거나 AutoML 옵션을 이용해서 모든 알고리즘을 사용해서 Training하고 그중에서 가장 높은 정확도를 가진 모델을 선택하는 방법도 있습니다. AutoML을 사용할 경우에는 더 많은 Training이 필요하므로 시간이 더 걸리고 과금도 더 되는데요. 맨 처음에 어떤 알고리즘을 사용해야 할지 모를때는 유용하게 쓰일 수 있을거 같습니다.

Forecast dimension에는 item_id별 예측 이외에 다른 필드별 예측을 하고 싶다면 Dataset에 명시된 필드값을 지정할 수 있습니다. 예를 들어서 아래와 같은 TARGET_TIME_SERIES 데이터가 있다고 가정할 때, 기본적으로 각 item_id별 수요가 예측될거고, Forecast dimension에 branch_id를 추가한다면 각 브랜치별 수요도 같이 예측할 수 있습니다.

    timestamp, branch_id, item_id, amount_sold

Country for holidays에는 특정 국가의 공휴일에 대한 데이터를 예측 모델에 적용할지를 묻고 있는데요. 코로나는 공휴일도 피해가지 않으니 저는 추가하지 않겠습니다.

Backset windows offer은... 말로 설명하기 쉽지 않네요. 우선 아래 그림을 보시죠.

![그림11](/assets/images/mediumBlog/20.04.12-foreCast/foreCast_11.png)

1월 1일부터 1월 31일까지의 데이터가 있다고 가정하겠습니다. Backtest windows offset은 얼마 만큼의 데이터를 Evaluation set으로 가져갈건지를 정합니다. Backtest windows offset 값을 7이라고 한다면 1월 1일부터 1월 24일까지의 데이터를 Training set으로 하고 1월 25일부터 1월 31일까지의 데이터를 Evaluation set으로 나눠서 예측 모델에 대한 정확도를 측정합니다. Backtest windows offset 값은  Forest horizion 값 보다 작을 수 없습니다.

Number of Backtest wikndows도... 아래 그림을 보시죠.

![그림12](/assets/images/mediumBlog/20.04.12-foreCast/foreCast_12.png)

다시 1월 1일부터 1월 31일 까지의 데이터가 있다고 가정하고, Number of Backtest windows를 4로, Backtest windows offset를 5로 하고 예측 모델을 생성한다면 아래와 같이 Dataset를 총 4개로 나눠서 예측 모델을 Training 합니다.

|iteration|training set|evaluation set|
|:---:|---|---|
|1|1월 1일 - 1월 26일|1월 27일 - 1월 31일|
|2|1월 1일 - 1월 21일|1월 22일 - 1월 26일|
|3|1월 1일 - 1월 16일|1월 17일 - 1월 21일|
|4|1월 1일 - 1월 11일|1월 12일 - 1월 16일|

그럼 예측 모델을 생성하겠습니다.

![그림13](/assets/images/mediumBlog/20.04.12-foreCast/foreCast_13.png)

생성이 완료 되었습니다.

![그림14](/assets/images/mediumBlog/20.04.12-foreCast/foreCast_14.png)

이제 생성된 예측 모델을 배포하겠습니다.

![그림15](/assets/images/mediumBlog/20.04.12-foreCast/foreCast_15.png)

Predictor에 배포할 예측 모델을 지정합니다. Forecast types은 우선 비워두고 Default값인 .10, .50, .90 quantiles로 예측결과를 받아보겠습니다. 이 부분도... 설명하기 힘드네요. 나중에 Query 결과를 통해서 알아보겠습니다.

![그림16](/assets/images/mediumBlog/20.04.12-foreCast/foreCast_16.png)

배포가 완료되고 Query를 실행해 보겠습니다.

![그림17](/assets/images/mediumBlog/20.04.12-foreCast/foreCast_17.png)

2020년 4월 8일부터 4월 12일까지 서울의 확진자 수를 예측해보겠습니다.

![그림18](/assets/images/mediumBlog/20.04.12-foreCast/foreCast_18.png)

Forecast 결과 값은 Probability Distribution를 기반으로 확율 예측 값입니다. 우선 P10, P50, P90 값들에 대해서 설명드리자면, P10는 10%의 확율로 실제값이 P10 값 보다 작을 수 있음을 의미하고, P50는 50%의 확율로 실제값이 P50 값 보다 크거나 작을 수 있음을 의미하고, P90는 90%의 확률로 실제값이 P90값 크지 않다는 것을 의미합니다.

그럼 여기서 P50에 의문이 들수도 있는데요. 왜냐하면 얼핏 보기에는 50/50 이기 떄문에 무작위 예측과 무엇이 다를 수 있냐고 볼 수도 있지만 실제는 다릅니다. 예측할 수 있는 값이 True/False 처럼 딱 2개라면 50%의 확률은 크게 의미가 없지만 예측 가능한 값이 무한대에 가깝다면 50%의 확률로 예측하는 것이 결코 나쁜 수치는 아닙니다.

그럼 이 P50 값과 실제 확인자 수를 비교해 보겠습니다.

|일자|예측값|실제값|
|---|---|---|
|2020-04-08|4.79|11(5/6)|
|2020-04-09|5.30|12(5/7)|
|2020-04-10|4.08|5(5/0)|
|2020-04-11|4.75|4(3/1)|
|2020-04-12|4.98|3(2/1)|

실제값 괄호안에 첫번째 값은 지역발생이고 두번째 값은 해외유입입니다. 지역 발생만 기준으로 본다면 나쁘지 않은 예측이 되었다고 볼수도 있겠네요. 사실 과거 확진자 발생 현황만으로 앞으로 발생할 확진자를 예측하는게 쉽지는 않습니다. 왜냐하면 수많은 변수들이 있을 수 있고 아직 코로나 바이러스 감염에 끼치는 영향도 명확히 밝혀지지 않았으니까요. 이번 예측은 그냥 재미로 한번 해봤다고 생각해 주시면 될 것 같습니다.

## Conclusion

이번 포스팅에 주된 목적은 Amazon Forecast에 대한 기본적인 사용법에 대해서 안내하는거였습니다. 다른 편에서는 Amazon Forecast에서 지원하는 알고리즘, 생성된 예측 모델의 지표값들 그리고 Probability Distribution에 대해서 알아보겠습니다.

참고로 저는 데이터 사이언티스트도 머신러닝 전문가도 아닌 그냥 평범한 개발자입니다. 혹시 이번 포스팅에서 잘못된 정보가 있으면 꼭 댓글 부탁드립니다.